{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "from nltk import PorterStemmer\n",
    "from rouge import Rouge\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.backward_freq = 16\n",
    "        self.max_length = 768\n",
    "        self.print_freq = 10000\n",
    "        self.model_save_name = \"D:\\Gradients\\seq2seq_models\\led\"\n",
    "        self.learning_rate = 3e-4\n",
    "        self.num_epochs = 1\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "s2s_args = ArgumentsS2S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELI5DatasetS2S(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_array,\n",
    "    ):\n",
    "        self.data = data_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def append(self, question_doc, answer):\n",
    "        self.data.append([question_doc, answer])\n",
    "\n",
    "    # def make_example(self, idx):\n",
    "    #     i, j = self.qa_id_list[idx]\n",
    "    #     example = self.data[i]\n",
    "    #     question = example[\"title\"] + \" \" + example[\"selftext\"]\n",
    "    #     answer = example[\"answers\"][\"text\"][j]\n",
    "    #     q_id = example[\"q_id\"]\n",
    "    #     if self.make_doc_function is not None:\n",
    "    #         self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"title\"]))\n",
    "    #     document = self.document_cache[q_id]\n",
    "    #     in_st = \"question: {} context: {}\".format(\n",
    "    #         question.lower().replace(\" --t--\", \"\").strip(), document.lower().strip(),\n",
    "    #     )\n",
    "    #     out_st = answer\n",
    "    #     return (in_st, out_st)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx][0], self.data[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n",
    "    q_ls = [q for q, a in qa_list]\n",
    "    a_ls = [a for q, a in qa_list]\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    lm_labels = a_ids[:, 1:].contiguous().clone()\n",
    "    lm_labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
    "    model_inputs = {\n",
    "        \"input_ids\": q_ids,\n",
    "        \"attention_mask\": q_mask,\n",
    "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
    "        \"labels\": lm_labels,\n",
    "    }\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_s2s_generate(\n",
    "    question_doc,\n",
    "    qa_s2s_model,\n",
    "    qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=None,\n",
    "    min_len=64,\n",
    "    max_len=256,\n",
    "    do_sample=False,\n",
    "    temp=1.0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n",
    "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
    "    generated_ids = qa_s2s_model.generate(\n",
    "        input_ids=model_inputs[\"input_ids\"],\n",
    "        attention_mask=model_inputs[\"attention_mask\"],\n",
    "        min_length=min_len,\n",
    "        max_length=max_len,\n",
    "        do_sample=do_sample,\n",
    "        early_stopping=True,\n",
    "        num_beams=1 if do_sample else n_beams,\n",
    "        temperature=temp,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_answers,\n",
    "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
    "    )\n",
    "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 82,466,176 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "model_id = \"flan-t5-small-lora\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-small\"\n",
    ").to(s2s_args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "inference_model = PeftModel.from_pretrained(model=model, model_id=model_id)\n",
    "inference_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file to close\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khoav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Users\\khoav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\khoav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "reference = []\n",
    "\n",
    "try:\n",
    "  f.close()\n",
    "except:\n",
    "  print(\"No file to close\")\n",
    "\n",
    "path = \"Bản sao của ELI5_val.jsonl\"\n",
    "f = open(path, \"r\")\n",
    "\n",
    "val_data = ELI5DatasetS2S([])\n",
    "\n",
    "for id, line in enumerate(f):\n",
    "  if id == 50:\n",
    "    break\n",
    "  # print(id)\n",
    "  data = json.loads(line)\n",
    "  # print(data)\n",
    "\n",
    "  question = data['question']\n",
    "  doc = '. '.join(map(str, data['ctxs']))\n",
    "  answer_true = '. '.join(map(str, data['answers']))\n",
    "\n",
    "  question_doc = \"question: {} context: {}\".format(question, doc)\n",
    "  answer_pred = qa_s2s_generate(\n",
    "            question_doc, inference_model, tokenizer,\n",
    "            num_answers=1,\n",
    "            num_beams=8,\n",
    "            min_len=96,\n",
    "            max_len=256,\n",
    "            max_input_length=512,\n",
    "            device=\"cuda:0\"\n",
    "    )[0]\n",
    "  predicted += [answer_pred]\n",
    "  reference += [answer_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dd91b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dd91b_level0_col0\" class=\"col_heading level0 col0\" >rouge1</th>\n",
       "      <th id=\"T_dd91b_level0_col1\" class=\"col_heading level0 col1\" >rouge2</th>\n",
       "      <th id=\"T_dd91b_level0_col2\" class=\"col_heading level0 col2\" >rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dd91b_level0_row0\" class=\"row_heading level0 row0\" >P</th>\n",
       "      <td id=\"T_dd91b_row0_col0\" class=\"data row0 col0\" >0.6292</td>\n",
       "      <td id=\"T_dd91b_row0_col1\" class=\"data row0 col1\" >0.2003</td>\n",
       "      <td id=\"T_dd91b_row0_col2\" class=\"data row0 col2\" >0.5882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dd91b_level0_row1\" class=\"row_heading level0 row1\" >R</th>\n",
       "      <td id=\"T_dd91b_row1_col0\" class=\"data row1 col0\" >0.0948</td>\n",
       "      <td id=\"T_dd91b_row1_col1\" class=\"data row1 col1\" >0.0206</td>\n",
       "      <td id=\"T_dd91b_row1_col2\" class=\"data row1 col2\" >0.0874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dd91b_level0_row2\" class=\"row_heading level0 row2\" >F</th>\n",
       "      <td id=\"T_dd91b_row2_col0\" class=\"data row2 col0\" >0.1580</td>\n",
       "      <td id=\"T_dd91b_row2_col1\" class=\"data row2 col1\" >0.0355</td>\n",
       "      <td id=\"T_dd91b_row2_col2\" class=\"data row2 col2\" >0.1461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x222f05c6c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "rouge = Rouge()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def compute_rouge_eli5(compare_list):\n",
    "    preds = [\" \".join([stemmer.stem(str(w))\n",
    "                       for w in tokenizer(pred)])\n",
    "             for gold, pred in compare_list]\n",
    "    golds = [\" \".join([stemmer.stem(str(w))\n",
    "                       for w in tokenizer(gold)])\n",
    "             for gold, pred in compare_list]\n",
    "    scores = rouge.get_scores(preds, golds, avg=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "compare_list = [(g, p) for p, g in zip(predicted, reference)]\n",
    "scores = compute_rouge_eli5(compare_list)\n",
    "df = pd.DataFrame({\n",
    "    'rouge1': [scores['rouge-1']['p'], scores['rouge-1']['r'], scores['rouge-1']['f']],\n",
    "    'rouge2': [scores['rouge-2']['p'], scores['rouge-2']['r'], scores['rouge-2']['f']],\n",
    "    'rougeL': [scores['rouge-l']['p'], scores['rouge-l']['r'], scores['rouge-l']['f']],\n",
    "}, index=[ 'P', 'R', 'F'])\n",
    "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
